{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "356ab395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ad55df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhicha/Project_Folder/Data Visualization Practicing/2notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9272d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4137, 3000) Test: (1035, 3000)\n",
      "Spam rate train: 0.290065264684554 Spam rate test: 0.2898550724637681\n"
     ]
    }
   ],
   "source": [
    "# Load the email data from a csv file and split into training and testing sets\n",
    "df = pd.read_csv(\"/Users/zhicha/Project_Folder/Data/emails.csv\")\n",
    "label_col = df.columns[-1]\n",
    "id_col = df.columns[0]\n",
    "feature_cols = df.columns[1:-1]\n",
    "\n",
    "X_raw = df[feature_cols]\n",
    "y = df[label_col]\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, test_size = 0.2, random_state = 42, stratify = y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train_raw.shape, \"Test:\", X_test_raw.shape)\n",
    "print(\"Spam rate train:\", y_train.mean(), \"Spam rate test:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b81b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After length filter: 2847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter short tokens\n",
    "cols_len3 = [c for c in X_train_raw.columns if len(str(c)) >= 3]\n",
    "\n",
    "X_train_len = X_train_raw[cols_len3]\n",
    "X_test_len = X_test_raw[cols_len3]\n",
    "print(\"After length filter:\", X_train_len.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5e33a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After DF filter: 1574\n"
     ]
    }
   ],
   "source": [
    "doc_freq = (X_train_len > 0).mean(axis=0)\n",
    "\n",
    "min_df = 0.01\n",
    "max_df = 0.90\n",
    "kept_cols = doc_freq[(doc_freq >= min_df) & (doc_freq <= max_df)].index.tolist()\n",
    "\n",
    "X_train = X_train_len[kept_cols]\n",
    "X_test = X_test_len[kept_cols]\n",
    "\n",
    "print(\"After DF filter:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da7cea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82cb23a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearSVM (tfidf)</td>\n",
       "      <td>0.970492</td>\n",
       "      <td>0.996685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg (tfidf)</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.995442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB (counts)</td>\n",
       "      <td>0.900929</td>\n",
       "      <td>0.967100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model        F1   ROC-AUC\n",
       "2       LinearSVM (tfidf)  0.970492  0.996685\n",
       "1          LogReg (tfidf)  0.962233  0.995442\n",
       "0  MultinomialNB (counts)  0.900929  0.967100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"MultinomialNB (counts)\": MultinomialNB(),\n",
    "    \"LogReg (tfidf)\": LogisticRegression(max_iter=2000),\n",
    "    \"LinearSVM (tfidf)\": LinearSVC()\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if \"counts\" in name:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "    else:\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            y_score = model.decision_function(X_test_tfidf)\n",
    "            auc = roc_auc_score(y_test, y_score)\n",
    "        else:\n",
    "            auc = np.nan\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results.append({\"model\": name, \"F1\": f1, \"ROC-AUC\": auc})\n",
    "    trained[name] = model\n",
    "\n",
    "results_df = pd.DataFrame(results, columns = [\"model\", \"F1\", \"ROC-AUC\"]).sort_values(\"F1\", ascending = False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc73cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3715411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "\n",
    "best_model_name = results_df.iloc[0][\"model\"]\n",
    "best_model = trained[best_model_name]\n",
    "\n",
    "if \"counts\" in best_model_name:\n",
    "    best_pred = best_model.predict(X_test)\n",
    "else:\n",
    "    best_pred = best_model.predict(X_test_tfidf)\n",
    "\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = [\"Not Spam\", \"Spam\"])\n",
    "disp.plot()\n",
    "plt.title(f\"Confusion Matrix: {best_model_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top words from logistic regression\n",
    "\n",
    "logreg = trained[\"LogReg (tfidf)\"]\n",
    "coefs = logreg.coef_.ravel()\n",
    "feature_names = np.array(kept_cols)\n",
    "\n",
    "top_n = 20\n",
    "top_spam_idx = np.argsort(coefs)[-top_n:]\n",
    "top_ham_idx = np.argsort(coefs)[:top_n]\n",
    "\n",
    "top_spam = pd.Series(coefs[top_spam_idx], index = feature_names[top_spam_idx]).sort_values()\n",
    "top_ham = pd.Series(coefs[top_ham_idx], index = feature_names[top_ham_idx]).sort_values()\n",
    "\n",
    "display(top_spam)\n",
    "display(top_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models_order = results_df[\"model\"].tolist()\n",
    "f1_vals = results_df[\"F1\"].tolist()\n",
    "auc_vals = results_df[\"ROC-AUC\"].tolist()\n",
    "\n",
    "x = np.arange(len(models_order))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - width/2, f1_vals, width, label=\"F1\")\n",
    "plt.bar(x + width/2, auc_vals, width, label=\"ROC-AUC\")\n",
    "plt.xticks(x, models_order, rotation=20, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Model Comparison (F1 & ROC-AUC)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for name, model in trained.items():\n",
    "    # 选对输入矩阵\n",
    "    X_in = X_test if \"counts\" in name else X_test_tfidf\n",
    "\n",
    "    # 取 score（用于ROC）\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_score = model.predict_proba(X_in)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_score = model.decision_function(X_in)\n",
    "    else:\n",
    "        continue  # 没有score就跳过（一般不会）\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "\n",
    "# 随机分类基线\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices for all models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = list(trained.keys())\n",
    "\n",
    "fig, axes = plt.subplots(1, len(names), figsize=(5*len(names), 4))\n",
    "\n",
    "if len(names) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, name in zip(axes, names):\n",
    "    model = trained[name]\n",
    "    X_in = X_test if \"counts\" in name else X_test_tfidf\n",
    "    y_pred = model.predict(X_in)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    im = ax.imshow(cm)\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([\"Not Spam\", \"Spam\"], rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels([\"Not Spam\", \"Spam\"])\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "    # 写数值\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "X2 = PCA(n_components=2, random_state=42).fit_transform(X_test_tfidf.toarray())\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y_test, s=10)\n",
    "plt.title(\"PCA (2D) projection of TF-IDF features\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
